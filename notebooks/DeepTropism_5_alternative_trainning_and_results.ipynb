{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and splitting Dataset\n",
    "The goal of this notebook is to continue the development of the DeepTropism model using a Pytorch.<br>\n",
    "The dataset was already created on the previous notebook and all the HIV-1 env V3 loop sequences where aligned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used on the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the Dataframe with all the sequences published on referenced articles.<br>\n",
    "The D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_aligned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RAB014775</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTGITIGPGQVWYRTGDIIGDIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTGI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RAB014776</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRQAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RAB014778</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RAB014781</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSVTIGPGQVWYRTGDIIGDIRQAYC</td>\n",
       "      <td>CTRPSNNT-RTSV---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RAB014834</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGNIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq_name dataset label                             sequence  \\\n",
       "0  RAB014775   newdb  CCR5  CTRPSNNTRTGITIGPGQVWYRTGDIIGDIRKAYC   \n",
       "1  RAB014776   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRQAYC   \n",
       "2  RAB014778   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRKAYC   \n",
       "3  RAB014781   newdb  CCR5  CTRPSNNTRTSVTIGPGQVWYRTGDIIGDIRQAYC   \n",
       "4  RAB014834   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGNIRKAYC   \n",
       "\n",
       "                                    sequence_aligned  label_numeric  \n",
       "0  CTRPSNNT-RTGI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "1  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "2  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "3  CTRPSNNT-RTSV---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "4  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/gabriel/Documents/Repos/DeepTropism/datasets/processed_tsv/alldataset_full_curated.tsv', \n",
    "                 sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the size of the column of the alig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{60}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['sequence_aligned'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9550, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.label != 'validation']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCR5     7705\n",
       "CXCR4     937\n",
       "R5X4      908\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7705\n",
       "1    1845\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the dataset by unique sequences\n",
    "To improve the quality of our trainning and avoid bias we are going to create a dataset with only unique sequences from the original Dataframe.\n",
    "This is going to be the dataset for the development of our model based on Deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates(subset=['sequence_aligned'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_aligned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RAB014775</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTGITIGPGQVWYRTGDIIGDIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTGI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>RAB014776</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRQAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RAB014778</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RAB014781</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSVTIGPGQVWYRTGDIIGDIRQAYC</td>\n",
       "      <td>CTRPSNNT-RTSV---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RAB014834</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRTSITIGPGQVWYRTGDIIGNIRKAYC</td>\n",
       "      <td>CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq_name dataset label                             sequence  \\\n",
       "0  RAB014775   newdb  CCR5  CTRPSNNTRTGITIGPGQVWYRTGDIIGDIRKAYC   \n",
       "1  RAB014776   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRQAYC   \n",
       "2  RAB014778   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGDIRKAYC   \n",
       "3  RAB014781   newdb  CCR5  CTRPSNNTRTSVTIGPGQVWYRTGDIIGDIRQAYC   \n",
       "4  RAB014834   newdb  CCR5  CTRPSNNTRTSITIGPGQVWYRTGDIIGNIRKAYC   \n",
       "\n",
       "                                    sequence_aligned  label_numeric  \n",
       "0  CTRPSNNT-RTGI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "1  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "2  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "3  CTRPSNNT-RTSV---T--I--G-P--G--Q-VWY---R-T--GDI...              0  \n",
       "4  CTRPSNNT-RTSI---T--I--G-P--G--Q-VWY---R-T--GDI...              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes for comparing performance against other methods\n",
    "In order to evaluate our model against the others already published, we are going to create separate Dataframes for each method filterinf by the 'dataset' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newdb         2998\n",
       "cm            2679\n",
       "hivcopred     2335\n",
       "geno2pheno    1188\n",
       "webpssm        350\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newdb = df[df.dataset == 'newdb']\n",
    "df_cm = df[df.dataset == 'cm']\n",
    "df_hivcopred = df[df.dataset == 'hivcopred']\n",
    "df_geno2pheno = df[df.dataset == 'geno2pheno']\n",
    "df_webpssm = df[df.dataset == 'webpssm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Dataset for Cross Validation\n",
    "We are going to create indices and set it to variables to make our cross validation reproducible. Our dataset is going to consist on:<br>\n",
    "* Trainning = 80 %\n",
    "* Validation = 10 %\n",
    "* Test = 10 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of indices and shuffle it using seed\n",
    "random.seed(42)\n",
    "size = df_unique.shape[0]\n",
    "list_indices = list(range(size))\n",
    "random.shuffle(list_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the list of indices for trainning, validation and test\n",
    "test_indices = list_indices[:int(size/10)]\n",
    "train_val_indices = list_indices[int(size/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_val_indices) + len(test_indices) == len(list_indices), \"Splitting indices with error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataloaders for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_sequence(protein_sequence):\n",
    "    \"\"\"\n",
    "    Function to convert a protein sequence into a tensor.\n",
    "    Each amino acid is represented by an numpy array of zeros of size 26,\n",
    "    and the dict_aa_pos defines the position to be converted to 1.\n",
    "    \n",
    "    The function iterates over the protein sequences and stacks the arrays.\n",
    "    At the end the arrays are linearized and converted to a tensor of size\n",
    "    n x 26, with n the size of the protein.\n",
    "    \n",
    "    If the character is not present on the dict_aa_pos (eg. '-') the respective\n",
    "    array is formed by zeros, and represents a missing value.\n",
    "    \"\"\"\n",
    "    dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}\n",
    "    \n",
    "    f_array = np.zeros(26)\n",
    "    for aa in protein_sequence:\n",
    "        arr = np.zeros(26)\n",
    "        if dict_aa_pos.get(aa):\n",
    "            arr[dict_aa_pos.get(aa)] = 1\n",
    "        f_array = np.vstack((f_array, arr))\n",
    "    f_array = np.delete(f_array, 0,0)\n",
    "    \n",
    "    #return torch.from_numpy((f_array.flatten()).astype(float))\n",
    "    return f_array.flatten().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data = []\n",
    "list_labels = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df.iterrows():\n",
    "    list_data.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels.append(int(row.label_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data) == len(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test set\n",
    "test_data = []\n",
    "test_label = []\n",
    "for j in test_indices:\n",
    "    test_data.append(list_data[j])\n",
    "    test_label.append(np.array(list_labels[j]))\n",
    "\n",
    "test_tensor_x = torch.stack([torch.from_numpy(i) for i in test_data]) # transform to torch tensors\n",
    "test_tensor_y = torch.stack([torch.from_numpy(i) for i in test_label])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y) # create your test dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we define the\n",
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_val_indices = np.array_split(train_val_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crossval_val_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "Cross Validation: 2\n",
      "Cross Validation: 3\n",
      "Cross Validation: 4\n",
      "Cross Validation: 5\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Deep Neural Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTropism_1(\n",
       "  (linear1): Linear(in_features=1560, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepTropism_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepTropism_1, self).__init__()\n",
    "        self.linear1 = nn.Linear(1560,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "model = DeepTropism_1().float()\n",
    "#model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTropism_2(\n",
       "  (linear1): Linear(in_features=1560, out_features=750, bias=True)\n",
       "  (linear2): Linear(in_features=750, out_features=250, bias=True)\n",
       "  (linear3): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear4): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepTropism_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepTropism_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(1560,750)\n",
    "        self.linear2 = nn.Linear(750,250)\n",
    "        self.linear3 = nn.Linear(250,100)\n",
    "        self.linear4 = nn.Linear(100,2)\n",
    "            \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = F.relu(self.linear3(X))\n",
    "        X = self.linear4(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "model_2 = DeepTropism_2().float()\n",
    "#model = model.float()\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "[1,   1] loss: 0.014180363\n",
      "Neural Network accuracy for validation set 1: 19.69%\n",
      "[21,   1] loss: 0.004136881\n",
      "Neural Network accuracy for validation set 1: 80.31%\n",
      "[41,   1] loss: 0.003257296\n",
      "Neural Network accuracy for validation set 1: 80.46%\n",
      "[61,   1] loss: 0.002938653\n",
      "Neural Network accuracy for validation set 1: 88.46%\n",
      "[81,   1] loss: 0.002780582\n",
      "Neural Network accuracy for validation set 1: 88.77%\n",
      "[101,   1] loss: 0.002657055\n",
      "Neural Network accuracy for validation set 1: 90.15%\n",
      "[121,   1] loss: 0.002561374\n",
      "Neural Network accuracy for validation set 1: 90.62%\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set DeepTropism_1\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    # Instantiante new model\n",
    "    #model = DeepTropism_1().float()\n",
    "        \n",
    "    # Define Cross Validation Trainning Loop\n",
    "    for epoch in range(400):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainning_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            #if i % 3239 == 0:    # print every 3239 mini-batches\n",
    "            if epoch % 20 == 0 and i % 3239 == 0:\n",
    "                print('[%d, %3d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in validation_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy for validation set {n + 1}: {round(100.0 * correct/total, 2)}%')\n",
    "            running_loss = 0.0\n",
    "             \n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    labels_array = np.empty([0])\n",
    "    predict_array = np.empty([0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            labels_array = np.concatenate([labels_array, labels])\n",
    "            predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            error += (predicted != labels).sum().item()\n",
    "\n",
    "    print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model_cv{n+1}.ptb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy for test set: 19.72%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy for test set: {round(100.0 * correct/total, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_true, y_score):\n",
    "    # True positive\n",
    "    tp = np.sum(y_true * y_score)\n",
    "    # False positive\n",
    "    fp = np.sum((y_true == 0) * y_score)\n",
    "    # True negative\n",
    "    tn = np.sum((y_true==0) * (y_score==0))\n",
    "    # False negative\n",
    "    fn = np.sum(y_true * (y_score==0))\n",
    "\n",
    "    # True positive rate (sensitivity or recall)\n",
    "    tpr = tp / (tp + fn)\n",
    "    # False positive rate (fall-out)\n",
    "    fpr = fp / (fp + tn)\n",
    "    # Precision\n",
    "    precision = tp / (tp + fp)\n",
    "    # True negatvie tate (specificity)\n",
    "    tnr = 1 - fpr\n",
    "    # F1 score\n",
    "    f1 = 2*tp / (2*tp + fp + fn)\n",
    "    # ROC-AUC for binary classification\n",
    "    auc = (tpr+tnr) / 2\n",
    "    # MCC\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    print(\"True positive: \", tp)\n",
    "    print(\"False positive: \", fp)\n",
    "    print(\"True negative: \", tn)\n",
    "    print(\"False negative: \", fn)\n",
    "\n",
    "    print(\"True positive rate (recall): \", tpr)\n",
    "    print(\"False positive rate: \", fpr)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"True negative rate: \", tnr)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"ROC-AUC: \", auc)\n",
    "    print(\"MCC: \", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:  60.0\n",
      "False positive:  11.0\n",
      "True negative:  264\n",
      "False negative:  22.0\n",
      "True positive rate (recall):  0.7317073170731707\n",
      "False positive rate:  0.04\n",
      "Precision:  0.8450704225352113\n",
      "True negative rate:  0.96\n",
      "F1:  0.7843137254901961\n",
      "ROC-AUC:  0.8458536585365853\n",
      "MCC:  0.7289260178853867\n"
     ]
    }
   ],
   "source": [
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "[1,   1] loss: 0.013994601\n",
      "Neural Network accuracy for validation set 1: 19.69%\n",
      "[21,   1] loss: 0.013994601\n",
      "Neural Network accuracy for validation set 1: 19.69%\n",
      "[41,   1] loss: 0.013994601\n",
      "Neural Network accuracy for validation set 1: 19.69%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0093bf835e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-da92144241ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set DeepTropism_2\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    # Instantiante new model\n",
    "    #model = DeepTropism_1().float()\n",
    "        \n",
    "    # Define Cross Validation Trainning Loop\n",
    "    for epoch in range(600):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainning_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model_2(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            #if i % 3239 == 0:    # print every 3239 mini-batches\n",
    "            if epoch % 20 == 0 and i % 3239 == 0:\n",
    "                print('[%d, %3d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in validation_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model_2(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy for validation set {n + 1}: {round(100.0 * correct/total, 2)}%')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    labels_array = np.empty([0])\n",
    "    predict_array = np.empty([0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model_2(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            labels_array = np.concatenate([labels_array, labels])\n",
    "            predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            error += (predicted != labels).sum().item()\n",
    "\n",
    "    print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "    show_metrics(labels_array, predict_array)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptropism",
   "language": "python",
   "name": "deeptropism"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
