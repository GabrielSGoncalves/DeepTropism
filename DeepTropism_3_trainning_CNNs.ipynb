{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and splitting Dataset\n",
    "The goal of this notebook is to continue the development of the DeepTropism model using a Pytorch.<br>\n",
    "The dataset was already created on the previous notebook and all the HIV-1 env V3 loop sequences where aligned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used on the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the Dataframe with all the sequences published on referenced articles.<br>\n",
    "The D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>B.FR.83.HXB2_LAI_IIIB_BRU.K03455</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPNNN-TRKRI-RIQRGPGRAFVTI-----GK-IGNMRQAHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A1.CD.02.LA01AlPr.KU168256</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CIRPNNN-TRKGI-GI--GPGQTFYAA-----DAIIGNIRHAYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1.CM.08.886_24.KP718928</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CSRPNNN-TRRSI-RI--GPGQSFYAT-----GEIIGDIREARC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A1.ES.15.100_117.KY496622</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPGNN-TRTSI-RI--GPGQAFYAT-----GDIIGDIRKAYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A1.KE.11.DEMA111KE002.KF716474</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPNNN-TRKSV-RI--GPGQAFFAT-----GEVIGKIRKAYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           seq_name dataset label  \\\n",
       "0  B.FR.83.HXB2_LAI_IIIB_BRU.K03455    LANL     -   \n",
       "1        A1.CD.02.LA01AlPr.KU168256    LANL     -   \n",
       "2          A1.CM.08.886_24.KP718928    LANL     -   \n",
       "3         A1.ES.15.100_117.KY496622    LANL     -   \n",
       "4    A1.KE.11.DEMA111KE002.KF716474    LANL     -   \n",
       "\n",
       "                               sequence_aligned  \n",
       "0  CTRPNNN-TRKRI-RIQRGPGRAFVTI-----GK-IGNMRQAHC  \n",
       "1  CIRPNNN-TRKGI-GI--GPGQTFYAA-----DAIIGNIRHAYC  \n",
       "2  CSRPNNN-TRRSI-RI--GPGQSFYAT-----GEIIGDIREARC  \n",
       "3  CTRPGNN-TRTSI-RI--GPGQAFYAT-----GDIIGDIRKAYC  \n",
       "4  CTRPNNN-TRKSV-RI--GPGQAFFAT-----GEVIGKIRKAYC  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/gabriel/Documents/Repos/DeepTropism/datasets/dataset_profile_final.tsv', \n",
    "                 sep='\\t', names=['seq_name', 'dataset', 'label', 'sequence_aligned'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9748, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the size of the column of the alig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{44}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['sequence_aligned'].apply(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the original datasets present on Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newdb         2998\n",
       "cm            2679\n",
       "hivcopred     2335\n",
       "geno2pheno    1188\n",
       "webpssm        350\n",
       "LANL           198\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9748, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.label != 'validation']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCR5     7705\n",
       "CXCR4     937\n",
       "R5X4      908\n",
       "-         198\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>B.FR.83.HXB2_LAI_IIIB_BRU.K03455</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPNNN-TRKRI-RIQRGPGRAFVTI-----GK-IGNMRQAHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A1.CD.02.LA01AlPr.KU168256</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CIRPNNN-TRKGI-GI--GPGQTFYAA-----DAIIGNIRHAYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A1.CM.08.886_24.KP718928</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CSRPNNN-TRRSI-RI--GPGQSFYAT-----GEIIGDIREARC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A1.ES.15.100_117.KY496622</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPGNN-TRTSI-RI--GPGQAFYAT-----GDIIGDIRKAYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A1.KE.11.DEMA111KE002.KF716474</td>\n",
       "      <td>LANL</td>\n",
       "      <td>-</td>\n",
       "      <td>CTRPNNN-TRKSV-RI--GPGQAFFAT-----GEVIGKIRKAYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9743</td>\n",
       "      <td>DUR.AM262127.O.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGDNSVKEMRA----GPMAWYSME--LERNGSRTNSRTAFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9744</td>\n",
       "      <td>DUR.X84327.O.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGNNSVQEIKI----GPMAWYSMQ--IEREGKGANSRTAFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9745</td>\n",
       "      <td>CCR5_AM262114_21502_FR_1995_O</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGSNSVQEIKI----GPMAWYSMQ--LEQDGKRANARTAFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9746</td>\n",
       "      <td>CXCR4/GPR15_NDK_13796_CD_1983_D</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>CTRPYKYTRQRTSI----GLRQSLYTI--TGKKKKTGYIGQAHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9747</td>\n",
       "      <td>CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>R5X4</td>\n",
       "      <td>CIREGIAEVQDIYT----GPMRWRSMT-LKRSNNTSPRSRVAYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9748 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               seq_name     dataset  label  \\\n",
       "0                      B.FR.83.HXB2_LAI_IIIB_BRU.K03455        LANL      -   \n",
       "1                            A1.CD.02.LA01AlPr.KU168256        LANL      -   \n",
       "2                              A1.CM.08.886_24.KP718928        LANL      -   \n",
       "3                             A1.ES.15.100_117.KY496622        LANL      -   \n",
       "4                        A1.KE.11.DEMA111KE002.KF716474        LANL      -   \n",
       "...                                                 ...         ...    ...   \n",
       "9743                                DUR.AM262127.O.CCR5          cm   CCR5   \n",
       "9744                                  DUR.X84327.O.CCR5          cm   CCR5   \n",
       "9745                      CCR5_AM262114_21502_FR_1995_O  geno2pheno   CCR5   \n",
       "9746                    CXCR4/GPR15_NDK_13796_CD_1983_D  geno2pheno  CXCR4   \n",
       "9747  CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...  geno2pheno   R5X4   \n",
       "\n",
       "                                  sequence_aligned  \n",
       "0     CTRPNNN-TRKRI-RIQRGPGRAFVTI-----GK-IGNMRQAHC  \n",
       "1     CIRPNNN-TRKGI-GI--GPGQTFYAA-----DAIIGNIRHAYC  \n",
       "2     CSRPNNN-TRRSI-RI--GPGQSFYAT-----GEIIGDIREARC  \n",
       "3     CTRPGNN-TRTSI-RI--GPGQAFYAT-----GDIIGDIRKAYC  \n",
       "4     CTRPNNN-TRKSV-RI--GPGQAFFAT-----GEVIGKIRKAYC  \n",
       "...                                            ...  \n",
       "9743  CVRPGDNSVKEMRA----GPMAWYSME--LERNGSRTNSRTAFC  \n",
       "9744  CVRPGNNSVQEIKI----GPMAWYSMQ--IEREGKGANSRTAFC  \n",
       "9745  CVRPGSNSVQEIKI----GPMAWYSMQ--LEQDGKRANARTAFC  \n",
       "9746  CTRPYKYTRQRTSI----GLRQSLYTI--TGKKKKTGYIGQAHC  \n",
       "9747  CIREGIAEVQDIYT----GPMRWRSMT-LKRSNNTSPRSRVAYC  \n",
       "\n",
       "[9748 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove samples from LANL of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = df[df.dataset != \"LANL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call labels\n",
    "def tropism_label(row):\n",
    "    \"\"\"\n",
    "    Define numeric label, 'CCR5' as 0 \n",
    "    and 'CXCR4' or 'R5X4' as 1\n",
    "    \"\"\"\n",
    "    # For CCR5\n",
    "    if str(row.label).strip() == 'CCR5':\n",
    "        return 0\n",
    "    # For CXCR4\n",
    "    elif str(row.label).strip() == 'CXCR4':\n",
    "        return 1\n",
    "    # For R5X4\n",
    "    elif str(row.label).strip() == 'R5X4':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/deeptropism/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_labeled['label_numeric'] = df_labeled.apply(tropism_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_aligned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>X138.EU074781.BG.CXCR4</td>\n",
       "      <td>cm</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>C-RPNN--TRKS------GPQ-----------YTIIGDIA---C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>CCR5_AG1030_-_FR_-_02_AG</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CSRPNNN-TRKSRI----GPGQTFYAT-----------DIGDQC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>CCR5_AG1005_-_FR_-_02_AG</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNN-TRKSIH----PGRAFYATV-----------GPQAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>-.FJ652339.02_AG.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRS--------VRIGPGQAF-------YAGDIGIQAC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>-.FJ375998.C.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CRPNNTRKMR--------IGPGQTYAT-------GDIIGIRAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9743</td>\n",
       "      <td>DUR.AM262127.O.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGDNSVKEMRA----GPMAWYSME--LERNGSRTNSRTAFC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9744</td>\n",
       "      <td>DUR.X84327.O.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGNNSVQEIKI----GPMAWYSMQ--IEREGKGANSRTAFC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9745</td>\n",
       "      <td>CCR5_AM262114_21502_FR_1995_O</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CVRPGSNSVQEIKI----GPMAWYSMQ--LEQDGKRANARTAFC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9746</td>\n",
       "      <td>CXCR4/GPR15_NDK_13796_CD_1983_D</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>CTRPYKYTRQRTSI----GLRQSLYTI--TGKKKKTGYIGQAHC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9747</td>\n",
       "      <td>CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>R5X4</td>\n",
       "      <td>CIREGIAEVQDIYT----GPMRWRSMT-LKRSNNTSPRSRVAYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9550 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               seq_name     dataset  label  \\\n",
       "198                              X138.EU074781.BG.CXCR4          cm  CXCR4   \n",
       "199                            CCR5_AG1030_-_FR_-_02_AG  geno2pheno   CCR5   \n",
       "200                            CCR5_AG1005_-_FR_-_02_AG  geno2pheno   CCR5   \n",
       "201                               -.FJ652339.02_AG.CCR5          cm   CCR5   \n",
       "202                                   -.FJ375998.C.CCR5          cm   CCR5   \n",
       "...                                                 ...         ...    ...   \n",
       "9743                                DUR.AM262127.O.CCR5          cm   CCR5   \n",
       "9744                                  DUR.X84327.O.CCR5          cm   CCR5   \n",
       "9745                      CCR5_AM262114_21502_FR_1995_O  geno2pheno   CCR5   \n",
       "9746                    CXCR4/GPR15_NDK_13796_CD_1983_D  geno2pheno  CXCR4   \n",
       "9747  CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...  geno2pheno   R5X4   \n",
       "\n",
       "                                  sequence_aligned  label_numeric  \n",
       "198   C-RPNN--TRKS------GPQ-----------YTIIGDIA---C              1  \n",
       "199   CSRPNNN-TRKSRI----GPGQTFYAT-----------DIGDQC              0  \n",
       "200   CTRPNNN-TRKSIH----PGRAFYATV-----------GPQAHC              0  \n",
       "201   CTRPNNNTRS--------VRIGPGQAF-------YAGDIGIQAC              0  \n",
       "202   CRPNNTRKMR--------IGPGQTYAT-------GDIIGIRAHC              0  \n",
       "...                                            ...            ...  \n",
       "9743  CVRPGDNSVKEMRA----GPMAWYSME--LERNGSRTNSRTAFC              0  \n",
       "9744  CVRPGNNSVQEIKI----GPMAWYSMQ--IEREGKGANSRTAFC              0  \n",
       "9745  CVRPGSNSVQEIKI----GPMAWYSMQ--LEQDGKRANARTAFC              0  \n",
       "9746  CTRPYKYTRQRTSI----GLRQSLYTI--TGKKKKTGYIGQAHC              1  \n",
       "9747  CIREGIAEVQDIYT----GPMRWRSMT-LKRSNNTSPRSRVAYC              1  \n",
       "\n",
       "[9550 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the dataset by unique sequences\n",
    "To improve the quality of our trainning and avoid bias we are going to create a dataset with only unique sequences from the original Dataframe.\n",
    "This is going to be the dataset for the development of our model based on Deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_labeled.drop_duplicates(subset=['sequence_aligned'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_aligned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>X138.EU074781.BG.CXCR4</td>\n",
       "      <td>cm</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>C-RPNN--TRKS------GPQ-----------YTIIGDIA---C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>CCR5_AG1030_-_FR_-_02_AG</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CSRPNNN-TRKSRI----GPGQTFYAT-----------DIGDQC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>CCR5_AG1005_-_FR_-_02_AG</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNN-TRKSIH----PGRAFYATV-----------GPQAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>-.FJ652339.02_AG.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRS--------VRIGPGQAF-------YAGDIGIQAC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>-.FJ375998.C.CCR5</td>\n",
       "      <td>cm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CRPNNTRKMR--------IGPGQTYAT-------GDIIGIRAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     seq_name     dataset  label  \\\n",
       "198    X138.EU074781.BG.CXCR4          cm  CXCR4   \n",
       "199  CCR5_AG1030_-_FR_-_02_AG  geno2pheno   CCR5   \n",
       "200  CCR5_AG1005_-_FR_-_02_AG  geno2pheno   CCR5   \n",
       "201     -.FJ652339.02_AG.CCR5          cm   CCR5   \n",
       "202         -.FJ375998.C.CCR5          cm   CCR5   \n",
       "\n",
       "                                 sequence_aligned  label_numeric  \n",
       "198  C-RPNN--TRKS------GPQ-----------YTIIGDIA---C              1  \n",
       "199  CSRPNNN-TRKSRI----GPGQTFYAT-----------DIGDQC              0  \n",
       "200  CTRPNNN-TRKSIH----PGRAFYATV-----------GPQAHC              0  \n",
       "201  CTRPNNNTRS--------VRIGPGQAF-------YAGDIGIQAC              0  \n",
       "202  CRPNNTRKMR--------IGPGQTYAT-------GDIIGIRAHC              0  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2783\n",
       "1     825\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCR5     2783\n",
       "R5X4      485\n",
       "CXCR4     340\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3608, 5)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes for comparing performance against other methods\n",
    "In order to evaluate our model against the others already published, we are going to create separate Dataframes for each method filterinf by the 'dataset' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newdb         2998\n",
       "cm            2679\n",
       "hivcopred     2335\n",
       "geno2pheno    1188\n",
       "webpssm        350\n",
       "Name: dataset, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newdb = df_labeled[df_labeled.dataset == 'newdb']\n",
    "df_cm = df_labeled[df_labeled.dataset == 'cm']\n",
    "df_hivcopred = df_labeled[df_labeled.dataset == 'hivcopred']\n",
    "df_geno2pheno = df_labeled[df_labeled.dataset == 'geno2pheno']\n",
    "df_webpssm = df_labeled[df_labeled.dataset == 'webpssm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2998, 5)\n",
      "(2679, 5)\n",
      "(2335, 5)\n",
      "(1188, 5)\n",
      "(350, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_newdb.shape)\n",
    "print(df_cm.shape)\n",
    "print(df_hivcopred.shape)\n",
    "print(df_geno2pheno.shape)\n",
    "print(df_webpssm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Dataset for Cross Validation\n",
    "We are going to create indices and set it to variables to make our cross validation reproducible. Our dataset is going to consist on:<br>\n",
    "* Trainning = 80 %\n",
    "* Validation = 10 %\n",
    "* Test = 10 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of indices and shuffle it using seed\n",
    "random.seed(42)\n",
    "size = df_unique.shape[0]\n",
    "list_indices = list(range(size))\n",
    "random.shuffle(list_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the list of indices for trainning, validation and test\n",
    "test_indices = list_indices[:int(size/10)]\n",
    "train_val_indices = list_indices[int(size/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_val_indices) + len(test_indices) == len(list_indices), \"Splitting indices with error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataloaders for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_sequence(protein_sequence):\n",
    "    \"\"\"\n",
    "    Function to convert a protein sequence into a tensor.\n",
    "    Each amino acid is represented by an numpy array of zeros of size 26,\n",
    "    and the dict_aa_pos defines the position to be converted to 1.\n",
    "    \n",
    "    The function iterates over the protein sequences and stacks the arrays.\n",
    "    At the end the arrays are linearized and converted to a tensor of size\n",
    "    n x 26, with n the size of the protein.\n",
    "    \n",
    "    If the character is not present on the dict_aa_pos (eg. '-') the respective\n",
    "    array is formed by zeros, and represents a missing value.\n",
    "    \"\"\"\n",
    "    dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}\n",
    "    \n",
    "    f_array = np.zeros(26)\n",
    "    for aa in protein_sequence:\n",
    "        arr = np.zeros(26)\n",
    "        if dict_aa_pos.get(aa):\n",
    "            arr[dict_aa_pos.get(aa)] = 1\n",
    "        f_array = np.vstack((f_array, arr))\n",
    "    f_array = np.delete(f_array, 0,0)\n",
    "    #return f_array.astype(float)\n",
    "    return f_array.flatten().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_aa_pos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data = []\n",
    "list_labels = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_unique.iterrows():\n",
    "    list_labels.append(int(row.label_numeric))\n",
    "    list_data.append(get_array_from_sequence(row.sequence_aligned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data) == len(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3608, 1144)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test set\n",
    "test_data = []\n",
    "test_label = []\n",
    "for j in test_indices:\n",
    "    test_data.append(list_data[j])\n",
    "    test_label.append(np.array(list_labels[j]))\n",
    "\n",
    "test_tensor_x = torch.stack([torch.from_numpy(i) for i in test_data]) # transform to torch tensors\n",
    "test_tensor_y = torch.stack([torch.from_numpy(i) for i in test_label])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y) # create your test dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we define the\n",
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_val_indices = np.array_split(train_val_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crossval_val_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "Cross Validation: 2\n",
      "Cross Validation: 3\n",
      "Cross Validation: 4\n",
      "Cross Validation: 5\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Deep Neural Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTropism_1(\n",
       "  (linear1): Linear(in_features=1144, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepTropism_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepTropism_1, self).__init__()\n",
    "        self.linear1 = nn.Linear(1144,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "\n",
    "model = DeepTropism_1().float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepTropism_2(\n",
       "  (linear1): Linear(in_features=1144, out_features=600, bias=True)\n",
       "  (linear2): Linear(in_features=600, out_features=300, bias=True)\n",
       "  (linear3): Linear(in_features=300, out_features=50, bias=True)\n",
       "  (linear4): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepTropism_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepTropism_2, self).__init__()\n",
    "        self.linear1 = nn.Linear(1144,600)\n",
    "        self.linear2 = nn.Linear(600,300)\n",
    "        self.linear3 = nn.Linear(300,50)\n",
    "        self.linear4 = nn.Linear(50,2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = F.relu(self.linear3(X))\n",
    "        X = self.linear4(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "\n",
    "model = DeepTropism_2().float()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For showing the metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_true, y_score):\n",
    "    # True positive\n",
    "    tp = np.sum(y_true * y_score)\n",
    "    # False positive\n",
    "    fp = np.sum((y_true == 0) * y_score)\n",
    "    # True negative\n",
    "    tn = np.sum((y_true==0) * (y_score==0))\n",
    "    # False negative\n",
    "    fn = np.sum(y_true * (y_score==0))\n",
    "\n",
    "    # True positive rate (sensitivity or recall)\n",
    "    tpr = tp / (tp + fn)\n",
    "    # False positive rate (fall-out)\n",
    "    fpr = fp / (fp + tn)\n",
    "    # Precision\n",
    "    precision = tp / (tp + fp)\n",
    "    # True negatvie tate (specificity)\n",
    "    tnr = 1 - fpr\n",
    "    # F1 score\n",
    "    f1 = 2*tp / (2*tp + fp + fn)\n",
    "    # ROC-AUC for binary classification\n",
    "    auc = (tpr+tnr) / 2\n",
    "    # MCC\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    print(\"True positive: \", tp)\n",
    "    print(\"False positive: \", fp)\n",
    "    print(\"True negative: \", tn)\n",
    "    print(\"False negative: \", fn)\n",
    "\n",
    "    print(\"True positive rate (recall): \", tpr)\n",
    "    print(\"False positive rate: \", fpr)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"True negative rate (Specificity): \", tnr)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"ROC-AUC: \", auc)\n",
    "    print(\"MCC: \", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "[1,   1] loss: 0.013544965\n",
      "Neural Network accuracy for validation set 1: 78.0%\n",
      "Neural Network accuracy on test set: 77.22%\n",
      "[21,   1] loss: 0.013421898\n",
      "Neural Network accuracy for validation set 1: 78.0%\n",
      "Neural Network accuracy on test set: 77.22%\n",
      "[41,   1] loss: 0.012217962\n",
      "Neural Network accuracy for validation set 1: 78.0%\n",
      "Neural Network accuracy on test set: 77.22%\n",
      "[61,   1] loss: 0.011039900\n",
      "Neural Network accuracy for validation set 1: 87.85%\n",
      "Neural Network accuracy on test set: 85.83%\n",
      "[81,   1] loss: 0.008590623\n",
      "Neural Network accuracy for validation set 1: 89.23%\n",
      "Neural Network accuracy on test set: 85.83%\n",
      "[101,   1] loss: 0.006424741\n",
      "Neural Network accuracy for validation set 1: 90.0%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[121,   1] loss: 0.005122365\n",
      "Neural Network accuracy for validation set 1: 90.62%\n",
      "Neural Network accuracy on test set: 86.94%\n",
      "[141,   1] loss: 0.004299819\n",
      "Neural Network accuracy for validation set 1: 90.46%\n",
      "Neural Network accuracy on test set: 86.94%\n",
      "[161,   1] loss: 0.003716545\n",
      "Neural Network accuracy for validation set 1: 90.31%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "[181,   1] loss: 0.003308861\n",
      "Neural Network accuracy for validation set 1: 90.15%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "Finished Training\n",
      "Cross Validation: 2\n",
      "[1,   1] loss: 0.006060724\n",
      "Neural Network accuracy for validation set 2: 92.77%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "[21,   1] loss: 0.005413849\n",
      "Neural Network accuracy for validation set 2: 92.0%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[41,   1] loss: 0.005015672\n",
      "Neural Network accuracy for validation set 2: 92.0%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[61,   1] loss: 0.004686237\n",
      "Neural Network accuracy for validation set 2: 91.85%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[81,   1] loss: 0.004368517\n",
      "Neural Network accuracy for validation set 2: 91.69%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[101,   1] loss: 0.003885130\n",
      "Neural Network accuracy for validation set 2: 91.38%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[121,   1] loss: 0.003311070\n",
      "Neural Network accuracy for validation set 2: 91.69%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[141,   1] loss: 0.002567832\n",
      "Neural Network accuracy for validation set 2: 91.38%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[161,   1] loss: 0.001977875\n",
      "Neural Network accuracy for validation set 2: 91.69%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[181,   1] loss: 0.001193249\n",
      "Neural Network accuracy for validation set 2: 91.54%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "Finished Training\n",
      "Cross Validation: 3\n",
      "[1,   1] loss: 0.000670109\n",
      "Neural Network accuracy for validation set 3: 95.69%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[21,   1] loss: 0.000558298\n",
      "Neural Network accuracy for validation set 3: 95.38%\n",
      "Neural Network accuracy on test set: 88.33%\n",
      "[41,   1] loss: 0.000364481\n",
      "Neural Network accuracy for validation set 3: 95.38%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[61,   1] loss: 0.000233661\n",
      "Neural Network accuracy for validation set 3: 95.23%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[81,   1] loss: 0.000158007\n",
      "Neural Network accuracy for validation set 3: 94.92%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[101,   1] loss: 0.000107811\n",
      "Neural Network accuracy for validation set 3: 94.62%\n",
      "Neural Network accuracy on test set: 87.22%\n",
      "[121,   1] loss: 0.000071524\n",
      "Neural Network accuracy for validation set 3: 94.77%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[141,   1] loss: 0.000050298\n",
      "Neural Network accuracy for validation set 3: 94.46%\n",
      "Neural Network accuracy on test set: 87.22%\n",
      "[161,   1] loss: 0.000033452\n",
      "Neural Network accuracy for validation set 3: 94.46%\n",
      "Neural Network accuracy on test set: 86.94%\n",
      "[181,   1] loss: 0.000021773\n",
      "Neural Network accuracy for validation set 3: 94.15%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "Finished Training\n",
      "Cross Validation: 4\n",
      "[1,   1] loss: 0.000017043\n",
      "Neural Network accuracy for validation set 4: 99.69%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[21,   1] loss: 0.000272190\n",
      "Neural Network accuracy for validation set 4: 94.45%\n",
      "Neural Network accuracy on test set: 85.28%\n",
      "[41,   1] loss: 0.000048942\n",
      "Neural Network accuracy for validation set 4: 97.07%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "[61,   1] loss: 0.000025858\n",
      "Neural Network accuracy for validation set 4: 97.53%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "[81,   1] loss: 0.000015241\n",
      "Neural Network accuracy for validation set 4: 97.53%\n",
      "Neural Network accuracy on test set: 86.67%\n",
      "[101,   1] loss: 0.000010779\n",
      "Neural Network accuracy for validation set 4: 97.38%\n",
      "Neural Network accuracy on test set: 87.22%\n",
      "[121,   1] loss: 0.000007750\n",
      "Neural Network accuracy for validation set 4: 97.23%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[141,   1] loss: 0.000005611\n",
      "Neural Network accuracy for validation set 4: 97.07%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[161,   1] loss: 0.000004317\n",
      "Neural Network accuracy for validation set 4: 97.07%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[181,   1] loss: 0.000003453\n",
      "Neural Network accuracy for validation set 4: 97.07%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "Finished Training\n",
      "Cross Validation: 5\n",
      "[1,   1] loss: 0.000002726\n",
      "Neural Network accuracy for validation set 5: 99.85%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[21,   1] loss: 0.000041195\n",
      "Neural Network accuracy for validation set 5: 94.92%\n",
      "Neural Network accuracy on test set: 83.33%\n",
      "[41,   1] loss: 0.000010106\n",
      "Neural Network accuracy for validation set 5: 98.77%\n",
      "Neural Network accuracy on test set: 87.5%\n",
      "[61,   1] loss: 0.000006435\n",
      "Neural Network accuracy for validation set 5: 98.77%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[81,   1] loss: 0.000004538\n",
      "Neural Network accuracy for validation set 5: 98.92%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[101,   1] loss: 0.000003456\n",
      "Neural Network accuracy for validation set 5: 98.77%\n",
      "Neural Network accuracy on test set: 88.06%\n",
      "[121,   1] loss: 0.000002635\n",
      "Neural Network accuracy for validation set 5: 98.77%\n",
      "Neural Network accuracy on test set: 87.78%\n",
      "[141,   1] loss: 0.000002034\n",
      "Neural Network accuracy for validation set 5: 98.61%\n",
      "Neural Network accuracy on test set: 87.22%\n",
      "[161,   1] loss: 0.000001617\n",
      "Neural Network accuracy for validation set 5: 98.61%\n",
      "Neural Network accuracy on test set: 86.94%\n",
      "[181,   1] loss: 0.000001350\n",
      "Neural Network accuracy for validation set 5: 98.61%\n",
      "Neural Network accuracy on test set: 86.94%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set DeepTropism_1\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    # Instantiante new model\n",
    "    #model = DeepTropism_1().float()\n",
    "        \n",
    "    # Define Cross Validation Trainning Loop\n",
    "    for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainning_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            #if i % 3239 == 0:    # print every 3239 mini-batches\n",
    "            if epoch % 20 == 0 and i % 3248 == 0:\n",
    "                print('[%d, %3d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in validation_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy for validation set {n + 1}: {round(100.0 * correct/total, 2)}%')\n",
    "                \n",
    "                # Evaluate model against test set\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in test_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            \n",
    "    torch.save(model.state_dict(), f'model_cv{n+1}.ptb')\n",
    "    print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy for test set: 87.22%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy for test set: {round(100.0 * correct/total, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:  61.0\n",
      "False positive:  25.0\n",
      "True negative:  253\n",
      "False negative:  21.0\n",
      "True positive rate (recall):  0.7439024390243902\n",
      "False positive rate:  0.08992805755395683\n",
      "Precision:  0.7093023255813954\n",
      "True negative rate (Specificity):  0.9100719424460432\n",
      "F1:  0.7261904761904762\n",
      "ROC-AUC:  0.8269871907352166\n",
      "MCC:  0.6432289060049595\n"
     ]
    }
   ],
   "source": [
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of DNN model against published datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence_aligned</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>C.ZW.01.TC30__phen_SI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>CTRPGNNTIGP-------GRTFYATDR-------IIGDIRQAHC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>DU179MAY00_ZA_C_SI/R5X4_u19_COETZER_(IN_PREP)</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>CTRPGNKTIRSIR-----LGPGQAFYT-------NKGDIRQASC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>DU179D_ZA_C_SI/CXCR4_u19_COETZER_(IN_PREP)</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CXCR4</td>\n",
       "      <td>CTRPGNKTIRSIR-----IGPGRTFYT-------NKGDIRQAYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>C.ZW.01.TC11__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRKSIW-----LGPGQAFYA-------NIIGDIRQAC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>C.MW.93.960__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNTRKSIRI-----GPGQTFYAT------NEIIGNREAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9525</td>\n",
       "      <td>C.ZA.98.TV018__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRRSMRI----RPGQTFYAT-----GEIIGDIRQAYC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9526</td>\n",
       "      <td>C.FR.92.FRMP130__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRRSVRI----GPGQTFYAT-----GAIIGDIRQAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9527</td>\n",
       "      <td>C.ZW.01.TC33__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPNNNTRTSVRI----GPGQAFYAT-----GDIIGDIRQAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9528</td>\n",
       "      <td>C.FR.93.FRMP37__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPSNNTRKSIRI----GPGQAFYAT-----NGIIGDIRAAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9529</td>\n",
       "      <td>C.FR.91.FRMP197__phen_NSI</td>\n",
       "      <td>webpssm</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTRPYNNTRQSIRI----GPGQTFYAT-----GDIIGDIRKAHC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           seq_name  dataset  label  \\\n",
       "225                           C.ZW.01.TC30__phen_SI  webpssm  CXCR4   \n",
       "263   DU179MAY00_ZA_C_SI/R5X4_u19_COETZER_(IN_PREP)  webpssm  CXCR4   \n",
       "264      DU179D_ZA_C_SI/CXCR4_u19_COETZER_(IN_PREP)  webpssm  CXCR4   \n",
       "302                          C.ZW.01.TC11__phen_NSI  webpssm   CCR5   \n",
       "466                           C.MW.93.960__phen_NSI  webpssm   CCR5   \n",
       "...                                             ...      ...    ...   \n",
       "9525                        C.ZA.98.TV018__phen_NSI  webpssm   CCR5   \n",
       "9526                      C.FR.92.FRMP130__phen_NSI  webpssm   CCR5   \n",
       "9527                         C.ZW.01.TC33__phen_NSI  webpssm   CCR5   \n",
       "9528                       C.FR.93.FRMP37__phen_NSI  webpssm   CCR5   \n",
       "9529                      C.FR.91.FRMP197__phen_NSI  webpssm   CCR5   \n",
       "\n",
       "                                  sequence_aligned  label_numeric  \n",
       "225   CTRPGNNTIGP-------GRTFYATDR-------IIGDIRQAHC              1  \n",
       "263   CTRPGNKTIRSIR-----LGPGQAFYT-------NKGDIRQASC              1  \n",
       "264   CTRPGNKTIRSIR-----IGPGRTFYT-------NKGDIRQAYC              1  \n",
       "302   CTRPNNNTRKSIW-----LGPGQAFYA-------NIIGDIRQAC              0  \n",
       "466   CTRPNNTRKSIRI-----GPGQTFYAT------NEIIGNREAHC              0  \n",
       "...                                            ...            ...  \n",
       "9525  CTRPNNNTRRSMRI----RPGQTFYAT-----GEIIGDIRQAYC              0  \n",
       "9526  CTRPNNNTRRSVRI----GPGQTFYAT-----GAIIGDIRQAHC              0  \n",
       "9527  CTRPNNNTRTSVRI----GPGQAFYAT-----GDIIGDIRQAHC              0  \n",
       "9528  CTRPSNNTRKSIRI----GPGQAFYAT-----NGIIGDIRAAHC              0  \n",
       "9529  CTRPYNNTRQSIRI----GPGQTFYAT-----GDIIGDIRKAHC              0  \n",
       "\n",
       "[350 rows x 5 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newdb\n",
    "df_cm\n",
    "df_hivcopred\n",
    "df_geno2pheno\n",
    "df_webpssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Newdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data_newdb = []\n",
    "list_labels_newdb = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_newdb.iterrows():\n",
    "    list_data_newdb.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels_newdb.append(np.array(int(row.label_numeric)))\n",
    "\n",
    "newdb_tensor_x = torch.stack([torch.from_numpy(i) for i in list_data_newdb]) # transform to torch tensors\n",
    "newdb_tensor_y = torch.stack([torch.from_numpy(i) for i in list_labels_newdb])\n",
    "\n",
    "newdb_dataset = torch.utils.data.TensorDataset(newdb_tensor_x,newdb_tensor_y) # create your test dataset\n",
    "newdb_dataloader = torch.utils.data.DataLoader(newdb_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on test set: 98.7%\n",
      "True positive:  640.0\n",
      "False positive:  16.0\n",
      "True negative:  2319\n",
      "False negative:  23.0\n",
      "True positive rate (recall):  0.9653092006033183\n",
      "False positive rate:  0.006852248394004282\n",
      "Precision:  0.975609756097561\n",
      "True negative rate (Specificity):  0.9931477516059957\n",
      "F1:  0.9704321455648218\n",
      "ROC-AUC:  0.979228476104657\n",
      "MCC:  0.962116036493835\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in newdb_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For CM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data_cm = []\n",
    "list_labels_cm = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_cm.iterrows():\n",
    "    list_data_cm.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels_cm.append(np.array(int(row.label_numeric)))\n",
    "\n",
    "cm_tensor_x = torch.stack([torch.from_numpy(i) for i in list_data_cm]) # transform to torch tensors\n",
    "cm_tensor_y = torch.stack([torch.from_numpy(i) for i in list_labels_cm])\n",
    "\n",
    "cm_dataset = torch.utils.data.TensorDataset(cm_tensor_x,cm_tensor_y) # create your test dataset\n",
    "cm_dataloader = torch.utils.data.DataLoader(cm_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2679, 1144])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_tensor_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on test set: 99.1%\n",
      "True positive:  322.0\n",
      "False positive:  21.0\n",
      "True negative:  2333\n",
      "False negative:  3.0\n",
      "True positive rate (recall):  0.9907692307692307\n",
      "False positive rate:  0.008920985556499575\n",
      "Precision:  0.9387755102040817\n",
      "True negative rate (Specificity):  0.9910790144435004\n",
      "F1:  0.9640718562874252\n",
      "ROC-AUC:  0.9909241226063656\n",
      "MCC:  0.9594134416903609\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in cm_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For hivcopred dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data_hivcopred = []\n",
    "list_labels_hivcopred = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_hivcopred.iterrows():\n",
    "    list_data_hivcopred.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels_hivcopred.append(np.array(int(row.label_numeric)))\n",
    "\n",
    "hivcopred_tensor_x = torch.stack([torch.from_numpy(i) for i in list_data_hivcopred]) # transform to torch tensors\n",
    "hivcopred_tensor_y = torch.stack([torch.from_numpy(i) for i in list_labels_hivcopred])\n",
    "\n",
    "hivcopred_dataset = torch.utils.data.TensorDataset(hivcopred_tensor_x,hivcopred_tensor_y) # create your test dataset\n",
    "hivcopred_dataloader = torch.utils.data.DataLoader(hivcopred_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on hivcopred set: 98.72%\n",
      "True positive:  551.0\n",
      "False positive:  14.0\n",
      "True negative:  1754\n",
      "False negative:  16.0\n",
      "True positive rate (recall):  0.9717813051146384\n",
      "False positive rate:  0.007918552036199095\n",
      "Precision:  0.9752212389380531\n",
      "True negative rate (Specificity):  0.9920814479638009\n",
      "F1:  0.9734982332155477\n",
      "ROC-AUC:  0.9819313765392197\n",
      "MCC:  0.9650215254489153\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in hivcopred_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy on hivcopred set: {round(100.0 * correct/total, 2)}%')\n",
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For geno2pheno dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data_geno2pheno = []\n",
    "list_labels_geno2pheno = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_geno2pheno.iterrows():\n",
    "    list_data_geno2pheno.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels_geno2pheno.append(np.array(int(row.label_numeric)))\n",
    "\n",
    "geno2pheno_tensor_x = torch.stack([torch.from_numpy(i) for i in list_data_geno2pheno]) # transform to torch tensors\n",
    "geno2pheno_tensor_y = torch.stack([torch.from_numpy(i) for i in list_labels_geno2pheno])\n",
    "\n",
    "geno2pheno_dataset = torch.utils.data.TensorDataset(geno2pheno_tensor_x,geno2pheno_tensor_y) # create your test dataset\n",
    "geno2pheno_dataloader = torch.utils.data.DataLoader(geno2pheno_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on hivcopred set: 97.98%\n",
      "True positive:  199.0\n",
      "False positive:  8.0\n",
      "True negative:  965\n",
      "False negative:  16.0\n",
      "True positive rate (recall):  0.9255813953488372\n",
      "False positive rate:  0.008221993833504625\n",
      "Precision:  0.961352657004831\n",
      "True negative rate (Specificity):  0.9917780061664954\n",
      "F1:  0.943127962085308\n",
      "ROC-AUC:  0.9586797007576663\n",
      "MCC:  0.931098205937677\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in geno2pheno_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy on hivcopred set: {round(100.0 * correct/total, 2)}%')\n",
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For webpssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data_webpssm = []\n",
    "list_labels_webpssm = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_webpssm.iterrows():\n",
    "    list_data_webpssm.append(get_array_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels_webpssm.append(np.array(int(row.label_numeric)))\n",
    "\n",
    "webpssm_tensor_x = torch.stack([torch.from_numpy(i) for i in list_data_webpssm]) # transform to torch tensors\n",
    "webpssm_tensor_y = torch.stack([torch.from_numpy(i) for i in list_labels_webpssm])\n",
    "\n",
    "webpssm_dataset = torch.utils.data.TensorDataset(webpssm_tensor_x,webpssm_tensor_y) # create your test dataset\n",
    "webpssm_dataloader = torch.utils.data.DataLoader(webpssm_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy on webpssm set: 95.71%\n",
      "True positive:  64.0\n",
      "False positive:  4.0\n",
      "True negative:  271\n",
      "False negative:  11.0\n",
      "True positive rate (recall):  0.8533333333333334\n",
      "False positive rate:  0.014545454545454545\n",
      "Precision:  0.9411764705882353\n",
      "True negative rate (Specificity):  0.9854545454545455\n",
      "F1:  0.8951048951048951\n",
      "ROC-AUC:  0.9193939393939394\n",
      "MCC:  0.8699015686885712\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in webpssm_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy on webpssm set: {round(100.0 * correct/total, 2)}%')\n",
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LeNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_sequence(protein_sequence):\n",
    "    \"\"\"\n",
    "    Function to convert a protein sequence into a tensor.\n",
    "    Each amino acid is represented by an numpy array of zeros of size 26,\n",
    "    and the dict_aa_pos defines the position to be converted to 1.\n",
    "    \n",
    "    The function iterates over the protein sequences and stacks the arrays.\n",
    "    At the end the arrays are linearized and converted to a tensor of size\n",
    "    n x 26, with n the size of the protein.\n",
    "    \n",
    "    If the character is not present on the dict_aa_pos (eg. '-') the respective\n",
    "    array is formed by zeros, and represents a missing value.\n",
    "    \"\"\"\n",
    "    dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}\n",
    "    \n",
    "    f_array = np.zeros(26)\n",
    "    for aa in protein_sequence:\n",
    "        arr = np.zeros(26)\n",
    "        if dict_aa_pos.get(aa):\n",
    "            arr[dict_aa_pos.get(aa)] = 1\n",
    "        f_array = np.vstack((f_array, arr))\n",
    "    f_array = np.delete(f_array, 0,0)\n",
    "    \n",
    "    #return torch.from_numpy((f_array.flatten()).astype(float))\n",
    "    return f_array.astype(float).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data = []\n",
    "list_labels = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_labeled.iterrows():\n",
    "    list_data.append(get_matrix_from_sequence(str(row.sequence_aligned)))\n",
    "    list_labels.append(int(row.label_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "Cross Validation: 2\n",
      "Cross Validation: 3\n",
      "Cross Validation: 4\n",
      "Cross Validation: 5\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 44)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matrix_from_sequence('CTRPNNNTRRSMRI----RPGQTFYAT-----GEIIGDIRQAYC').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(26, 3), stride=(1, 1))\n",
       "  (max_pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (max_pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the network (LeNet-5)  \n",
    "class LeNet5(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):   \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(26,3), stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = torch.nn.Linear(84, 2)        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_1(x)\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "     \n",
    "#net = LeNet5()\n",
    "model = LeNet5().float()\n",
    "#model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model = Net().float()\n",
    "#model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 6 1 3, but got 3-dimensional input of size [64, 26, 44] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-e18810556146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-1150f42ad6e2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Max pooling over a (2, 2) window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 6 1 3, but got 3-dimensional input of size [64, 26, 44] instead"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set DeepTropism_1\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    # Instantiante new model\n",
    "    #model = DeepTropism_1().float()\n",
    "        \n",
    "    # Define Cross Validation Trainning Loop\n",
    "    for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainning_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            #inputs = inputs.unsqueeze(0)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            #if i % 3239 == 0:    # print every 3239 mini-batches\n",
    "            if epoch % 20 == 0 and i % 3248 == 0:\n",
    "                print('[%d, %3d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in validation_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy for validation set {n + 1}: {round(100.0 * correct/total, 2)}%')\n",
    "                \n",
    "                # Evaluate model against test set\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                error = 0\n",
    "                labels_array = np.empty([0])\n",
    "                predict_array = np.empty([0])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for data in test_dataloader:\n",
    "                        images, labels = data\n",
    "                        outputs = model(images.float())\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        labels_array = np.concatenate([labels_array, labels])\n",
    "                        predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        error += (predicted != labels).sum().item()\n",
    "\n",
    "                print(f'Neural Network accuracy on test set: {round(100.0 * correct/total, 2)}%')\n",
    "\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            \n",
    "    torch.save(model.state_dict(), f'model_cv{n+1}.ptb')\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras (TensorFlow) to train a CNN for DeepTropism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_sequence(protein_sequence):\n",
    "    \"\"\"\n",
    "    Function to convert a protein sequence into a tensor.\n",
    "    Each amino acid is represented by an numpy array of zeros of size 26,\n",
    "    and the dict_aa_pos defines the position to be converted to 1.\n",
    "    \n",
    "    The function iterates over the protein sequences and stacks the arrays.\n",
    "    At the end the arrays are linearized and converted to a tensor of size\n",
    "    n x 26, with n the size of the protein.\n",
    "    \n",
    "    If the character is not present on the dict_aa_pos (eg. '-') the respective\n",
    "    array is formed by zeros, and represents a missing value.\n",
    "    \"\"\"\n",
    "    dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}\n",
    "    \n",
    "    f_array = np.zeros(26)\n",
    "    for aa in protein_sequence:\n",
    "        arr = np.zeros(26)\n",
    "        if dict_aa_pos.get(aa):\n",
    "            arr[dict_aa_pos.get(aa)] = 1\n",
    "        f_array = np.vstack((f_array, arr))\n",
    "    f_array = np.delete(f_array, 0,0)\n",
    "    #return f_array.astype(float)\n",
    "    return f_array.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 26)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_matrix_from_sequence('CTRPNNN-TRKRI-RIQRGPGRAFVTI-----GK-IGNMRQAHC').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data = []\n",
    "list_labels = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df_unique.iterrows():\n",
    "    list_labels.append(int(row.label_numeric))\n",
    "    list_data.append(get_matrix_from_sequence(row.sequence_aligned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3608, 44, 26)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "Cross Validation: 2\n",
      "Cross Validation: 3\n",
      "Cross Validation: 4\n",
      "Cross Validation: 5\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 38, 38)            6954      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 9, 38)             0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 342)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                5488      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 12,476\n",
      "Trainable params: 12,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=38, kernel_size=7 ,\n",
    "                 input_shape=(44, 26)))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (3608, 1) was passed for an output of shape (None, 2) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-7a8a5258b2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(np.array(list_data), list_labels, \n\u001b[0;32m----> 2\u001b[0;31m                     epochs=50, verbose=0, validation_split=0.25)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2690\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2692\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeptropism/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    547\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    548\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (3608, 1) was passed for an output of shape (None, 2) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(list_data), list_labels, \n",
    "                    epochs=50, verbose=0, validation_split=0.25)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 26)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list_data)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptropism",
   "language": "python",
   "name": "deeptropism"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
