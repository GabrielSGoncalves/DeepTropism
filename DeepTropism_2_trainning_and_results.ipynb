{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and splitting Dataset\n",
    "The goal of this notebook is to continue the development of the DeepTropism model using a Pytorch.<br>\n",
    "The dataset was already created on the previous notebook and all the HIV-1 env V3 loop sequences where aligned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used on the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the Dataframe with all the sequences published on referenced articles.<br>\n",
    "The D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CCR5_1471_29187_CN_2003_B</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CTQTQQQY-KKKY---T----------SRTR-ASM-----V-CNRR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...</td>\n",
       "      <td>geno2pheno</td>\n",
       "      <td>R5X4</td>\n",
       "      <td>CIREGIAE-VQDI---Y--T--G-P-----M-RWRSMTLKR-SNNT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>RKF859742</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CERPTMDI-QDIH------I--G-P-----M-AWYSTYIER-QAKG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>RAF009608</td>\n",
       "      <td>hivcopred</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CSRPEMDV-QEIR---N-----G-P-----M-AWYSMALAK-GGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>RKF859743</td>\n",
       "      <td>newdb</td>\n",
       "      <td>CCR5</td>\n",
       "      <td>CRRPAMKV-QEMR---I----------G--PMAWY-----S-MALE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name     dataset label  \\\n",
       "0                          CCR5_1471_29187_CN_2003_B  geno2pheno  CCR5   \n",
       "1  CCR5/CXCR4/CCR1/CCR2b/CCR3/CCR4_MVP5180_67_CM_...  geno2pheno  R5X4   \n",
       "2                                          RKF859742       newdb  CCR5   \n",
       "3                                          RAF009608   hivcopred  CCR5   \n",
       "4                                          RKF859743       newdb  CCR5   \n",
       "\n",
       "                                            sequence  \n",
       "0  CTQTQQQY-KKKY---T----------SRTR-ASM-----V-CNRR...  \n",
       "1  CIREGIAE-VQDI---Y--T--G-P-----M-RWRSMTLKR-SNNT...  \n",
       "2  CERPTMDI-QDIH------I--G-P-----M-AWYSTYIER-QAKG...  \n",
       "3  CSRPEMDV-QEIR---N-----G-P-----M-AWYSMALAK-GGTT...  \n",
       "4  CRRPAMKV-QEMR---I----------G--PMAWY-----S-MALE...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/gabriel/Documents/Repos/DeepTropism/dataset_unique_seqs_aligned_gapopen_15_old.tsv', \n",
    "                 sep='\\t', names=['name', 'dataset', 'label','sequence'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3572, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.label != 'validation']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call labels\n",
    "def tropism_label(row):\n",
    "    # For CCR5\n",
    "    if row.label == 'CCR5':\n",
    "        return 0\n",
    "    # For CXCR4\n",
    "    elif row.label == 'CXCR4':\n",
    "        return 1\n",
    "    # For R5X4\n",
    "    elif row.label == 'R5X4':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_numeric'] = df.apply(tropism_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CCR5     2757\n",
       "R5X4      484\n",
       "CXCR4     331\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2757\n",
       "1     815\n",
       "Name: label_numeric, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_numeric.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Dataset for Cross Validation\n",
    "We are going to create indices and set it to variables to make our cross validation reproducible. Our dataset is going to consist on:<br>\n",
    "* Trainning = 80 %\n",
    "* Validation = 10 %\n",
    "* Test = 10 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of indices and shuffle it using seed\n",
    "random.seed(23)\n",
    "size = df.shape[0]\n",
    "list_indices = list(range(size))\n",
    "random.shuffle(list_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the list of indices for trainning, validation and test\n",
    "test_indices = list_indices[:int(size/10)]\n",
    "train_val_indices = list_indices[int(size/10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_val_indices) + len(test_indices) == len(list_indices), \"Splitting indices with error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataloaders for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_sequence(protein_sequence):\n",
    "    \"\"\"\n",
    "    Function to convert a protein sequence into a tensor.\n",
    "    Each amino acid is represented by an numpy array of zeros of size 26,\n",
    "    and the dict_aa_pos defines the position to be converted to 1.\n",
    "    \n",
    "    The function iterates over the protein sequences and stacks the arrays.\n",
    "    At the end the arrays are linearized and converted to a tensor of size\n",
    "    n x 26, with n the size of the protein.\n",
    "    \n",
    "    If the character is not present on the dict_aa_pos (eg. '-') the respective\n",
    "    array is formed by zeros, and represents a missing value.\n",
    "    \"\"\"\n",
    "    dict_aa_pos = {\n",
    "    'A':1, 'R':2, 'N':3, 'D':4, 'C':5, 'Q':6, 'E':7, 'G':8,\n",
    "    'H':9, 'I':10, 'L':11, 'K':12, 'M':13, 'F':14, 'P':15, \n",
    "    'O':16, 'S':17, 'U':18, 'T':19, 'W':20, 'Y':21, 'V':22, \n",
    "    'B':23, 'Z':24, 'J':25, 'X':0}\n",
    "    \n",
    "    f_array = np.zeros(26)\n",
    "    for aa in protein_sequence:\n",
    "        arr = np.zeros(26)\n",
    "        if dict_aa_pos.get(aa):\n",
    "            arr[dict_aa_pos.get(aa)] = 1\n",
    "        f_array = np.vstack((f_array, arr))\n",
    "    f_array = np.delete(f_array, 0,0)\n",
    "    \n",
    "    #return torch.from_numpy((f_array.flatten()).astype(float))\n",
    "    return f_array.flatten().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to append data from the df\n",
    "list_data = []\n",
    "list_labels = []\n",
    "\n",
    "# Convert the sequences and labels to arrays to use as data on pytorch\n",
    "for index, row in df.iterrows():\n",
    "    list_data.append(get_array_from_sequence(str(row.sequence)))\n",
    "    list_labels.append(int(row.label_numeric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_data) == len(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test set\n",
    "test_data = []\n",
    "test_label = []\n",
    "for j in test_indices:\n",
    "    test_data.append(list_data[j])\n",
    "    test_label.append(np.array(list_labels[j]))\n",
    "\n",
    "test_tensor_x = torch.stack([torch.from_numpy(i) for i in test_data]) # transform to torch tensors\n",
    "test_tensor_y = torch.stack([torch.from_numpy(i) for i in test_label])\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y) # create your test dataset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64) # create your dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3215"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we define the\n",
    "len(train_val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_val_indices = np.array_split(train_val_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crossval_val_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "Cross Validation: 2\n",
      "Cross Validation: 3\n",
      "Cross Validation: 4\n",
      "Cross Validation: 5\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Deep Neural Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepTropism_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepTropism_1, self).__init__()\n",
    "        self.linear1 = nn.Linear(1560,250)\n",
    "        self.linear2 = nn.Linear(250,100)\n",
    "        self.linear3 = nn.Linear(100,2)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "model = DeepTropism_1().float()\n",
    "#model = model.float()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation: 1\n",
      "[1,         1] loss: 0.00004\n",
      "[21,         1] loss: 0.00001\n",
      "[41,         1] loss: 0.00000\n",
      "[61,         1] loss: 0.00000\n",
      "[81,         1] loss: 0.00000\n",
      "[101,         1] loss: 0.00000\n",
      "[121,         1] loss: 0.00000\n",
      "[141,         1] loss: 0.00000\n",
      "[161,         1] loss: 0.00000\n",
      "[181,         1] loss: 0.00000\n",
      "[201,         1] loss: 0.00000\n",
      "[221,         1] loss: 0.00000\n",
      "[241,         1] loss: 0.00000\n",
      "[261,         1] loss: 0.00000\n",
      "[281,         1] loss: 0.00000\n",
      "[301,         1] loss: 0.00000\n",
      "[321,         1] loss: 0.00000\n",
      "[341,         1] loss: 0.00000\n",
      "[361,         1] loss: 0.00000\n",
      "[381,         1] loss: 0.00000\n",
      "[401,         1] loss: 0.00000\n",
      "[421,         1] loss: 0.00000\n",
      "[441,         1] loss: 0.00000\n",
      "[461,         1] loss: 0.00000\n",
      "[481,         1] loss: 0.00000\n",
      "[501,         1] loss: 0.00000\n",
      "[521,         1] loss: 0.00000\n",
      "[541,         1] loss: 0.00000\n",
      "[561,         1] loss: 0.00000\n",
      "[581,         1] loss: 0.00000\n",
      "Finished Training\n",
      "Neural Network accuracy Cross Validation 0: 95.02%\n",
      "Cross Validation: 2\n",
      "[1,         1] loss: 0.00002\n",
      "[21,         1] loss: 0.00000\n",
      "[41,         1] loss: 0.00000\n",
      "[61,         1] loss: 0.00000\n",
      "[81,         1] loss: 0.00000\n",
      "[101,         1] loss: 0.00000\n",
      "[121,         1] loss: 0.00000\n",
      "[141,         1] loss: 0.00000\n",
      "[161,         1] loss: 0.00000\n",
      "[181,         1] loss: 0.00000\n",
      "[201,         1] loss: 0.00000\n",
      "[221,         1] loss: 0.00000\n",
      "[241,         1] loss: 0.00000\n",
      "[261,         1] loss: 0.00000\n",
      "[281,         1] loss: 0.00000\n",
      "[301,         1] loss: 0.00000\n",
      "[321,         1] loss: 0.00000\n",
      "[341,         1] loss: 0.00000\n",
      "[361,         1] loss: 0.00000\n",
      "[381,         1] loss: 0.00000\n",
      "[401,         1] loss: 0.00000\n",
      "[421,         1] loss: 0.00000\n",
      "[441,         1] loss: 0.00000\n",
      "[461,         1] loss: 0.00000\n",
      "[481,         1] loss: 0.00000\n",
      "[501,         1] loss: 0.00000\n",
      "[521,         1] loss: 0.00000\n",
      "[541,         1] loss: 0.00000\n",
      "[561,         1] loss: 0.00000\n",
      "[581,         1] loss: 0.00000\n",
      "Finished Training\n",
      "Neural Network accuracy Cross Validation 1: 99.69%\n",
      "Cross Validation: 3\n",
      "[1,         1] loss: 0.00000\n",
      "[21,         1] loss: 0.00000\n",
      "[41,         1] loss: 0.00000\n",
      "[61,         1] loss: 0.00000\n",
      "[81,         1] loss: 0.00000\n",
      "[101,         1] loss: 0.00000\n",
      "[121,         1] loss: 0.00000\n",
      "[141,         1] loss: 0.00000\n",
      "[161,         1] loss: 0.00000\n",
      "[181,         1] loss: 0.00000\n",
      "[201,         1] loss: 0.00000\n",
      "[221,         1] loss: 0.00000\n",
      "[241,         1] loss: 0.00000\n",
      "[261,         1] loss: 0.00000\n",
      "[281,         1] loss: 0.00000\n",
      "[301,         1] loss: 0.00000\n",
      "[321,         1] loss: 0.00000\n",
      "[341,         1] loss: 0.00000\n",
      "[361,         1] loss: 0.00000\n",
      "[381,         1] loss: 0.00000\n",
      "[401,         1] loss: 0.00000\n",
      "[421,         1] loss: 0.00000\n",
      "[441,         1] loss: 0.00000\n",
      "[461,         1] loss: 0.00000\n",
      "[481,         1] loss: 0.00000\n",
      "[501,         1] loss: 0.00000\n",
      "[521,         1] loss: 0.00000\n",
      "[541,         1] loss: 0.00000\n",
      "[561,         1] loss: 0.00000\n",
      "[581,         1] loss: 0.00000\n",
      "Finished Training\n",
      "Neural Network accuracy Cross Validation 2: 100.0%\n",
      "Cross Validation: 4\n",
      "[1,         1] loss: 0.00000\n",
      "[21,         1] loss: 0.00000\n",
      "[41,         1] loss: 0.00000\n",
      "[61,         1] loss: 0.00000\n",
      "[81,         1] loss: 0.00000\n",
      "[101,         1] loss: 0.00000\n",
      "[121,         1] loss: 0.00000\n",
      "[141,         1] loss: 0.00000\n",
      "[161,         1] loss: 0.00000\n",
      "[181,         1] loss: 0.00000\n",
      "[201,         1] loss: 0.00000\n",
      "[221,         1] loss: 0.00000\n",
      "[241,         1] loss: 0.00000\n",
      "[261,         1] loss: 0.00000\n",
      "[281,         1] loss: 0.00000\n",
      "[301,         1] loss: 0.00000\n",
      "[321,         1] loss: 0.00000\n",
      "[341,         1] loss: 0.00000\n",
      "[361,         1] loss: 0.00000\n",
      "[381,         1] loss: 0.00000\n",
      "[401,         1] loss: 0.00000\n",
      "[421,         1] loss: 0.00000\n",
      "[441,         1] loss: 0.00000\n",
      "[461,         1] loss: 0.00000\n",
      "[481,         1] loss: 0.00000\n",
      "[501,         1] loss: 0.00000\n",
      "[521,         1] loss: 0.00000\n",
      "[541,         1] loss: 0.00000\n",
      "[561,         1] loss: 0.00000\n",
      "[581,         1] loss: 0.00000\n",
      "Finished Training\n",
      "Neural Network accuracy Cross Validation 3: 100.0%\n",
      "Cross Validation: 5\n",
      "[1,         1] loss: 0.00000\n",
      "[21,         1] loss: 0.00000\n",
      "[41,         1] loss: 0.00000\n",
      "[61,         1] loss: 0.00000\n",
      "[81,         1] loss: 0.00000\n",
      "[101,         1] loss: 0.00000\n",
      "[121,         1] loss: 0.00000\n",
      "[141,         1] loss: 0.00000\n",
      "[161,         1] loss: 0.00000\n",
      "[181,         1] loss: 0.00000\n",
      "[201,         1] loss: 0.00000\n",
      "[221,         1] loss: 0.00000\n",
      "[241,         1] loss: 0.00000\n",
      "[261,         1] loss: 0.00000\n",
      "[281,         1] loss: 0.00000\n",
      "[301,         1] loss: 0.00000\n",
      "[321,         1] loss: 0.00000\n",
      "[341,         1] loss: 0.00000\n",
      "[361,         1] loss: 0.00000\n",
      "[381,         1] loss: 0.00000\n",
      "[401,         1] loss: 0.00000\n",
      "[421,         1] loss: 0.00000\n",
      "[441,         1] loss: 0.00000\n",
      "[461,         1] loss: 0.00000\n",
      "[481,         1] loss: 0.00000\n",
      "[501,         1] loss: 0.00000\n",
      "[521,         1] loss: 0.00000\n",
      "[541,         1] loss: 0.00000\n",
      "[561,         1] loss: 0.00000\n",
      "[581,         1] loss: 0.00000\n",
      "Finished Training\n",
      "Neural Network accuracy Cross Validation 4: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# For Trainning and validation set\n",
    "# Define the cross validation indices for trainning and validation sets\n",
    "crossval_val_indices = np.array_split(train_val_indices,5)\n",
    "\n",
    "# Iterate over crossval_val_indices defining the Dataloaders\n",
    "for n in range(len(crossval_val_indices)):\n",
    "    print(f'Cross Validation: {n + 1}')\n",
    "    trainning_data = []\n",
    "    trainning_label = []\n",
    "    validation_data = []\n",
    "    validation_label = []\n",
    "\n",
    "    validation_indices = list(crossval_val_indices[n])\n",
    "    trainning_indices = list(set(train_val_indices) - set(validation_indices))\n",
    "\n",
    "    for j in validation_indices:\n",
    "        validation_data.append(list_data[j])\n",
    "        validation_label.append(np.array(list_labels[j]))\n",
    "\n",
    "    validation_tensor_x = torch.stack([torch.from_numpy(i) for i in validation_data]) # transform to torch tensors\n",
    "    validation_tensor_y = torch.stack([torch.from_numpy(i) for i in validation_label])\n",
    "\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_tensor_x,validation_tensor_y) # create your test dataset\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    for k in trainning_indices:\n",
    "        trainning_data.append(list_data[k])\n",
    "        trainning_label.append(np.array(list_labels[k]))\n",
    "\n",
    "    trainning_tensor_x = torch.stack([torch.from_numpy(i) for i in trainning_data]) # transform to torch tensors\n",
    "    trainning_tensor_y = torch.stack([torch.from_numpy(i) for i in trainning_label])\n",
    "\n",
    "    trainning_dataset = torch.utils.data.TensorDataset(trainning_tensor_x,trainning_tensor_y) # create your test dataset\n",
    "    trainning_dataloader = torch.utils.data.DataLoader(trainning_dataset, batch_size=64) # create your dataloader\n",
    "    \n",
    "    # Instantiante new model\n",
    "    #model = DeepTropism_1().float()\n",
    "        \n",
    "    # Define Cross Validation Trainning Loop\n",
    "    for epoch in range(600):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainning_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            #print(running_loss)\n",
    "            #if i % 3239 == 0:    # print every 3239 mini-batches\n",
    "            if epoch % 20 == 0 and i % 3239 == 0:\n",
    "                print('[%d, %3d] loss: %.9f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    error = 0\n",
    "    labels_array = np.empty([0])\n",
    "    predict_array = np.empty([0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            labels_array = np.concatenate([labels_array, labels])\n",
    "            predict_array = np.concatenate([predict_array, predicted])\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            error += (predicted != labels).sum().item()\n",
    "\n",
    "    print(f'Neural Network accuracy Cross Validation {n}: {round(100.0 * correct/total, 2)}%')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model_cv{n+1}.ptb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network accuracy: 91.04%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "error = 0\n",
    "labels_array = np.empty([0])\n",
    "predict_array = np.empty([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images.float())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        labels_array = np.concatenate([labels_array, labels])\n",
    "        predict_array = np.concatenate([predict_array, predicted])\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        error += (predicted != labels).sum().item()\n",
    "\n",
    "print(f'Neural Network accuracy: {round(100.0 * correct/total, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(y_true, y_score):\n",
    "    # True positive\n",
    "    tp = np.sum(y_true * y_score)\n",
    "    # False positive\n",
    "    fp = np.sum((y_true == 0) * y_score)\n",
    "    # True negative\n",
    "    tn = np.sum((y_true==0) * (y_score==0))\n",
    "    # False negative\n",
    "    fn = np.sum(y_true * (y_score==0))\n",
    "\n",
    "    # True positive rate (sensitivity or recall)\n",
    "    tpr = tp / (tp + fn)\n",
    "    # False positive rate (fall-out)\n",
    "    fpr = fp / (fp + tn)\n",
    "    # Precision\n",
    "    precision = tp / (tp + fp)\n",
    "    # True negatvie tate (specificity)\n",
    "    tnr = 1 - fpr\n",
    "    # F1 score\n",
    "    f1 = 2*tp / (2*tp + fp + fn)\n",
    "    # ROC-AUC for binary classification\n",
    "    auc = (tpr+tnr) / 2\n",
    "    # MCC\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    print(\"True positive: \", tp)\n",
    "    print(\"False positive: \", fp)\n",
    "    print(\"True negative: \", tn)\n",
    "    print(\"False negative: \", fn)\n",
    "\n",
    "    print(\"True positive rate (recall): \", tpr)\n",
    "    print(\"False positive rate: \", fpr)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"True negative rate: \", tnr)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"ROC-AUC: \", auc)\n",
    "    print(\"MCC: \", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:  62.0\n",
      "False positive:  12.0\n",
      "True negative:  263\n",
      "False negative:  20.0\n",
      "True positive rate (recall):  0.7560975609756098\n",
      "False positive rate:  0.04363636363636364\n",
      "Precision:  0.8378378378378378\n",
      "True negative rate:  0.9563636363636363\n",
      "F1:  0.7948717948717948\n",
      "ROC-AUC:  0.856230598669623\n",
      "MCC:  0.7393080105538697\n"
     ]
    }
   ],
   "source": [
    "show_metrics(labels_array, predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptropism",
   "language": "python",
   "name": "deeptropism"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
